{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NLP Pipeline - Part 2\n",
    "As we mentioned in the lecture slides, an NLP pipeline is constructed from  the following steps: \n",
    "- Data acquisition, \n",
    "- Text extraction and cleaning \n",
    "- Pre-processing\n",
    "- Feature Engineering\n",
    "- Modelling\n",
    "- Evaluation\n",
    "- Deployement\n",
    "- Monitoring & Model updating\n",
    "\n",
    "In this notebook, we will try to explain how to convert pre-processed text into a numerical representation using the Bag Of Word (BOW) technique. Then fine different classification models will be created and evaluated to classify tweets into positive or negative sentiments. Next, you will learn how to evaluate different models and select the best model for future use. \n",
    "\n",
    "You can open the cloud version of this notebook using the following link:\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Ali-Alameer/NLP/blob/main/week3_pipeline_part2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-processed data\n",
    "\n",
    "Last week you stored the pre-processed tweets and labels in the CSV file. We need to load the file to continue our process of exploring different stages of the NLP pipeline.\n",
    "\n",
    "Following code will read the CSV file using Pandas library and will load data into a datafram. A <b>Pandas DataFrame</b> is a 2 dimensional data structure, like a 2 dimensional array, or a table with rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data generated in the previous workshop\n",
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Ali-Alameer/NLP/main/data/processed_train_tweets.csv'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('processed_train_tweets.csv', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print('CSV file downloaded successfully!')\n",
    "else:\n",
    "    print(f'Failed to download CSV file. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading CSV files\n",
    "train_df = pd.read_csv('processed_train_tweets.csv', index_col = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let to check the loaded data by displaying the first 5 tweets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows of training data\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how the data is structured, let's take a look at it. There will be a result showing how many rows and columns the dataset contains by printing the shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of positive and negative tweets using the value_counts() method of a dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sentiment.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset description indicates that:\n",
    "- <b>0</b> ==> <b>positive sentiments</b>\n",
    "- <b>1</b> ==> <b>negative sentiments</b>\n",
    "\n",
    "According to the result of the previous cell, there are 29,720 positive tweets and 2,242 negative tweets in the training dataset. As a result, the training dataset is <b>imbalanced</b> since the data points are not equal for the two classes.\n",
    "\n",
    "For storing sentiments, a Python dictionary is an appropriate data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary to map numbers to corresponding sentiments\n",
    "map = {0: 'Positive', 1: 'Negative'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Remember the input data is just a bunch of words for now. Machine learning models only understand and work on numbers. So we have to convert all the text into numbers. In NLP, this conversion of raw text to a suitable numerical form is called <b>text representation</b>. You will learn about this process in the following  lectures. \n",
    "\n",
    "Now that input is clean and ready, we convert it into numbers using something called the <b>\"Bag of Words\"</b> approach. We create a matrix table, where each row represents a sentence and each word will have a separate column for itself that represents its frequency. Let’s take an example, to make it easier. Let’s say there are 3 tweets like: \n",
    "- The food is really bad \n",
    "- The food is really good  \n",
    "- The food is really good and tasty. \n",
    "\n",
    "This could be represented as a matrix using the bag of words approach as shown:\n",
    "\n",
    "| Tweet Number | The | food | is | bad | really | good | tasty | and |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | \n",
    "| 1 |  1 |  1 |  1 |  1 |  1 |  0 |  0 |  0 | \n",
    "| 2 |  1 |  1 |  1 |  0 |  1 |  1 |  0 |  0 | \n",
    "| 3 |  1 |  1 |  1 |  0 |  1 |  1 |  1 |  1 | \n",
    "\n",
    "One problem about this method that you might not notice is that the order of the sentence is lost. There are other approaches to counter this, but we are just going to stick with this method at the moment. You will learn more about other approaches in the following workshops. \n",
    "\n",
    "The CountVectorizer class in the \"sklearn.feature_extraction.text\" module will be utilised to generate Bag of words for our tweets dataset.The Count Vectorizer function converts a list of words into a bag of words, however, notice that we specify something called the max features to it. As you might have seen in the bag of words illustration table, each word will have a separate column. This number of columns can explode into large numbers in big datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a count vertorizer object and set the size of the vocabulary to 8000\n",
    "cv = CountVectorizer(max_features = 8000, ngram_range = (1,2))\n",
    "\n",
    "# convert the dtype of final tweet column to unicode string and convert them to bag of words\n",
    "X = cv.fit_transform(train_df['final_tweet'].values.astype('U')).toarray()\n",
    "y = train_df['sentiment']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this we set the max columns as 8000, and keep the maximum occurring 8000 words. Finally, the `cv.fit_transform` function takes the cleaned data and converts it into the bag of words that we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X: \", X[0:5])\n",
    "print(\"y: \",y[0:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "One of the first decisions to make when starting a modeling phase is how to utilize the existing data. One common technique is to split the data into two groups typically referred to as the `training` and `testing` sets. The training set is used to train a model and the test set is used to evaluate the model. \n",
    "\n",
    "The `train_test_split` class of `sklearn.model_selection` class has been used to generate the training and test set in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(\"number of tweets in training dataset: \", len(X_train))\n",
    "print(\"number of tweets in testing dataset: \", len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we’ll build classifiers using five well-known machine learning algorithms: \n",
    "- Multinomial Naive Bayes,\n",
    "- Bernoulli Naive Bayes, \n",
    "- logistic regression, \n",
    "- Random Forest, and \n",
    "- Decision Tree \n",
    "\n",
    "You don't need to worry about these models as you will learn detailed implementation of them in the `Machine Learning` module next semester. \n",
    "\n",
    "In the following section, we will import the required libraries to create our classifiers. To evaluate the models after they have been trained, a list of evaluation metrics is also imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an object from each of the classifiers model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "bnb_model = BernoulliNB()\n",
    "lr_model = LogisticRegression(max_iter = 300, multi_class = \"multinomial\")\n",
    "rfc_model = RandomForestClassifier(n_estimators = 50, random_state = 2, max_depth = 25)\n",
    "tree_model = DecisionTreeClassifier(max_depth = 30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's to creat a function to train a model and get the confusion metrix of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_get_confusion_metric(model, X_train, y_train, X_test, y_test):\n",
    "    # train the model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate the performance metrics\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return confusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will train only the logistic regression model and print the confution metrix of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_confusion = train_classifier_get_confusion_metric(lr_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Confusion Matrix: \", lr_confusion)\n",
    "disp = ConfusionMatrixDisplay(lr_confusion, display_labels = map.keys())\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Exercise</font>\n",
    "\n",
    "1- Calculate the accuracy of the liner regression model.\n",
    "\n",
    "<div>\n",
    "<img src=\"data:image/png;base64,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\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy of the model\n",
    "# lr_accuracy = ? \n",
    "# print(lr_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Exercise</font>\n",
    "\n",
    "1- Calculate the percision of the liner regression model.\n",
    "\n",
    "<div>\n",
    "<img src=\"data:image/png;base64,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\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the precision of the model\n",
    "# lr_precision = ?\n",
    "# print(lr_precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the code the following code defined a function that trains and returns the accuracy, and precision scores of the trained model. We also created a dictionary to store our model names. Then using a for loop we will call the function to train and store the socres for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, X_train, y_train, X_test, y_test):\n",
    "    # train the model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate the performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, precision, confusion, recall, f1\n",
    "\n",
    "# create a dictionary of models\n",
    "models = {\n",
    "    'Multinomial Naive Bayes': mnb_model,\n",
    "    'Bernoulli Naive Bayes': bnb_model,\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest Classifier': rfc_model,\n",
    "    'Decision Tree Classifier': tree_model\n",
    "}\n",
    "\n",
    "# create a list to store performance of models\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "confusions = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # train the model on training data\n",
    "    cur_accuracy, cur_precision, cur_confusion, cur_recall, cur_f1 = train_classifier(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # print model performance\n",
    "    print(\"Model: \" , model_name)\n",
    "    print(\"Accuracy: \", cur_accuracy)\n",
    "    print(\"Precision: \", cur_precision)\n",
    "    print(\"Recall: \", cur_recall)\n",
    "    print(\"F1: \", cur_f1)\n",
    "    print(\"Confusion Matrix: \", cur_confusion)\n",
    "    disp = ConfusionMatrixDisplay(cur_confusion, display_labels = map.keys())\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "    # append the performance metrics to a list\n",
    "    accuracy_scores.append(cur_accuracy)\n",
    "    precision_scores.append(cur_precision)\n",
    "    confusions.append(cur_confusion)\n",
    "    recall_scores.append(cur_recall)\n",
    "    f1_scores.append(cur_f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "In order to show the performance of all models together a dataframe is created and displayed in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame({'Algorithm': models.keys(), \n",
    "                                'Accuracy': accuracy_scores, \n",
    "                                'Precision': precision_scores, \n",
    "                                'Recall': recall_scores, \n",
    "                                'F1': f1_scores,\n",
    "                                'Confusion': confusions}).sort_values(\"Accuracy\", ascending = False, ignore_index = True)\n",
    "\n",
    "performance_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all other models, we can see that Logistic regression performs the best with an accuracy score of 95% followed by Multinomail Naive Bayes. As it is a classification task so we will not take recall and precision into consideration because here false-positive and false-negative don’t concern us.\n",
    "\n",
    "Finally, we can save the model using a pickle file so that we can deploy it into the deployment phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"./best_model/LogisticRegression.pickle\"\n",
    "pickle.dump(lr_model, open(file_name, \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86cd70f776a547a286cf4ac2da0f2330b91abb11d3736f6a2d32a6533d7c646d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
