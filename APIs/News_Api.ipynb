{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# News Api"
      ],
      "metadata": {
        "id": "xF-b3UhBkTcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the [News Api](https://newsapi.org/) we can retrieve articles from a various range of subjects and from all around the world. Although this Api limits us in the quantity of words that we can retrieve from each article, pairing it with a web scrapper like Beautifulsoup allows us to retrieve the entirety of those news articles.\n",
        "\n",
        "This Api can be particularly useful in the case of topic modelling or text classification."
      ],
      "metadata": {
        "id": "k78_Q8jGQIHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by installing the necessary libraries for this example.\n",
        "\n",
        "*Beautifulsoup* will allow us to scrap the articles web pages and *the newsapi-python* is a Python client library for the News Api. The github page of the library can be found here: https://github.com/mattlisiv/newsapi-python."
      ],
      "metadata": {
        "id": "hlgbvh7upGSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install newsapi-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V54haOzhHW39",
        "outputId": "f78f2a6a-e57f-4f1c-b700-0c70d5a7612d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from newsapi-python) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2024.2.2)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The very first step will be to create an account on the [News Api](https://newsapi.org/) website. Once the account created, you will be able to retrieve an Api Key on your account page. Assign the Api Key to the *api_key* variable.\n",
        "\n",
        "Since we are under a *Developper* subscription we only have access to a 100 Api calls per day. But this should be way enough to test and retrieve all the informations we want.\n",
        "\n",
        "The next step is to change the search criteria to the main subject treated by the news articles you are looking for.\n",
        "\n",
        "There is some other parameters available for the search queries, such as the language in which the news article is written. A full list of those parameters is available here: https://newsapi.org/docs/client-libraries/python."
      ],
      "metadata": {
        "id": "CIaQUD7HqMlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-_t2gJtAn53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82f313a-afbb-4d50-b0e6-706555deedf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of articles :  100  out of  321\n",
            "Number of articles :  200  out of  321\n",
            "Number of articles :  300  out of  321\n",
            "Number of articles :  321  out of  321\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from newsapi import NewsApiClient\n",
        "\n",
        "# Lists that will hold the content of the articles\n",
        "titles = []\n",
        "authors = []\n",
        "sources = []\n",
        "contents = []\n",
        "\n",
        "# Copy and paste your Api key here\n",
        "api_key = ''\n",
        "\n",
        "# Authentication process facilitated by the library\n",
        "newsapi = NewsApiClient(api_key=api_key)\n",
        "\n",
        "# The different parameters for the Api request\n",
        "q = 'neuralink'\n",
        "language = 'en'\n",
        "page = 1\n",
        "\n",
        "# An article count that serve us to keep a count of how many articles we've retrieved thanks to the request\n",
        "articleCount = 100\n",
        "\n",
        "while (articleCount == 100):\n",
        "  # Return the list and content of articles in the defined page of our search\n",
        "  articles = newsapi.get_everything(q=q, language=language, page=page)\n",
        "  newsDf = pd.DataFrame(articles)\n",
        "  articleCount = len(newsDf)\n",
        "  print('Number of articles : ', articleCount + 100 * (page - 1), ' out of ', newsDf['totalResults'][1])\n",
        "  for article in newsDf['articles']:\n",
        "    # We need a try catch in the case an exception is raised due to connection aborted issues\n",
        "    try:\n",
        "      # Retrieve the html code of the news article page\n",
        "      response = requests.get(article['url'])\n",
        "      # Initialize the beautifulsoup html parser\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      # We are looking for the first element with an 'article' tag on the page and continue if we can't find any\n",
        "      result = soup.find('article')\n",
        "      if not result:\n",
        "        continue\n",
        "      # Now we are looking for all elements with a 'p' tag inside the result of the previous search\n",
        "      texts = result.find_all('p')\n",
        "      if not texts:\n",
        "        continue\n",
        "      articleContent = ''\n",
        "      # We loop through all the article elements to retrieve the entirety of its content\n",
        "      for text in texts:\n",
        "        articleContent = articleContent + text.text\n",
        "      # A consuming check but we have to verify the article returned actually contains the content of the search\n",
        "      if q.lower() not in articleContent.lower() and q.lower() not in article['title'].lower():\n",
        "        continue\n",
        "      # At this point we should have the content of the article, we can add all the important informations to our lists\n",
        "      contents.append(articleContent)\n",
        "      titles.append(article['title'])\n",
        "      authors.append(article['author'])\n",
        "      sources.append(article['source']['name'])\n",
        "    except Exception:\n",
        "      pass\n",
        "  page += 1\n",
        "\n",
        "# We create a dataframe from the content lists that we have\n",
        "finalDf = pd.DataFrame({\n",
        "    'article':contents,\n",
        "    'title': titles,\n",
        "    'author':authors,\n",
        "    'source':sources\n",
        "})\n",
        "\n",
        "# This part produce a csv file and then compress it into a zip to facilitate the file download and transfer\n",
        "compression_opts = dict(method='zip', archive_name='News_Api_Articles.csv')\n",
        "finalDf.to_csv('News_Api_Articles.zip', index=False, compression=compression_opts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZ-5wmFYjF35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}