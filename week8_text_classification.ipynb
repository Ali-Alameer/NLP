{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Text Classification with Movie Reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Ali-Alameer/NLP/blob/main/week8_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem. \n",
    "\n",
    "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
    "\n",
    "This notebook uses [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrk8NjzhSBh-"
   },
   "source": [
    "### More models\n",
    "[Here](https://tfhub.dev/s?module-type=text-embedding) you can find more expressive or performant models that you could use to generate the text embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4DN769E2O_R"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: tensorflow in /home/zeeshan/.local/lib/python3.10/site-packages (2.15.0.post1)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (3.10.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.3)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (23.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (4.25.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (4.9.0)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (0.36.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (1.62.0)\r\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (2.15.2)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow) (2.15.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/zeeshan/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/zeeshan/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zeeshan/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.5)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2020.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/zeeshan/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\r\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/zeeshan/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting tensorflow_hub\r\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.26.3)\r\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow_hub) (4.25.3)\r\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow_hub)\r\n",
      "  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\r\n",
      "Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tf-keras, tensorflow_hub\r\n",
      "Successfully installed tensorflow_hub-0.16.1 tf-keras-2.15.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting tensorflow_datasets\r\n",
      "  Downloading tensorflow_datasets-4.9.4-py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: absl-py in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow_datasets) (2.1.0)\r\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from tensorflow_datasets) (8.0.3)\r\n",
      "Collecting dm-tree (from tensorflow_datasets)\r\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\r\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\r\n",
      "  Downloading etils-1.7.0-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.26.3)\r\n",
      "Collecting promise (from tensorflow_datasets)\r\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: protobuf>=3.20 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow_datasets) (4.25.3)\r\n",
      "Requirement already satisfied: psutil in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow_datasets) (5.9.8)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow_datasets) (2.31.0)\r\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\r\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Requirement already satisfied: termcolor in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow_datasets) (2.4.0)\r\n",
      "Collecting toml (from tensorflow_datasets)\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Collecting tqdm (from tensorflow_datasets)\r\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.6/57.6 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: wrapt in /home/zeeshan/.local/lib/python3.10/site-packages (from tensorflow_datasets) (1.14.1)\r\n",
      "Collecting array-record>=0.5.0 (from tensorflow_datasets)\r\n",
      "  Downloading array_record-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (503 bytes)\r\n",
      "Collecting fsspec (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\r\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting importlib_resources (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets)\r\n",
      "  Downloading importlib_resources-6.1.2-py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Requirement already satisfied: typing_extensions in /home/zeeshan/.local/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.9.0)\r\n",
      "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (1.0.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zeeshan/.local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.5)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from promise->tensorflow_datasets) (1.16.0)\r\n",
      "Collecting absl-py (from tensorflow_datasets)\r\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\r\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\r\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\r\n",
      "Downloading tensorflow_datasets-4.9.4-py3-none-any.whl (5.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.1/5.1 MB\u001B[0m \u001B[31m10.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading array_record-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading etils-1.7.0-py3-none-any.whl (152 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m152.4/152.4 kB\u001B[0m \u001B[31m18.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m152.8/152.8 kB\u001B[0m \u001B[31m8.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\r\n",
      "Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.5/126.5 kB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m16.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.3/78.3 kB\u001B[0m \u001B[31m16.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m228.7/228.7 kB\u001B[0m \u001B[31m19.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m170.9/170.9 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading importlib_resources-6.1.2-py3-none-any.whl (34 kB)\r\n",
      "Building wheels for collected packages: promise\r\n",
      "  Building wheel for promise (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=3e8dbbda46af96b4c9689b13505184bce9cfcbc37cc21eb6e483132470fa33dc\r\n",
      "  Stored in directory: /home/zeeshan/.cache/pip/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\r\n",
      "Successfully built promise\r\n",
      "Installing collected packages: dm-tree, tqdm, toml, protobuf, promise, importlib_resources, fsspec, etils, absl-py, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow_datasets\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 4.25.3\r\n",
      "    Uninstalling protobuf-4.25.3:\r\n",
      "      Successfully uninstalled protobuf-4.25.3\r\n",
      "  Attempting uninstall: absl-py\r\n",
      "    Found existing installation: absl-py 2.1.0\r\n",
      "    Uninstalling absl-py-2.1.0:\r\n",
      "      Successfully uninstalled absl-py-2.1.0\r\n",
      "Successfully installed absl-py-1.4.0 array-record-0.5.0 dm-tree-0.1.8 etils-1.7.0 fsspec-2024.2.0 googleapis-common-protos-1.62.0 importlib_resources-6.1.2 promise-2.3 protobuf-3.20.3 tensorflow-metadata-1.14.0 tensorflow_datasets-4.9.4 toml-0.10.2 tqdm-4.66.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting matplotlib\r\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\r\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib)\r\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\r\n",
      "  Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m159.1/159.1 kB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\r\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeeshan/.local/lib/python3.10/site-packages (from matplotlib) (23.2)\r\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/zeeshan/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m17.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m310.7/310.7 kB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.6/4.6 MB\u001B[0m \u001B[31m20.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\r\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.49.0 kiwisolver-1.4.5 matplotlib-3.8.3\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub\n",
    "!pip install tensorflow_datasets\n",
    "!pip install matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T09:31:25.182172949Z",
     "start_time": "2024-02-27T09:31:14.403692671Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2ew7HTbPpCJH",
    "vscode": {
     "languageId": "python"
    },
    "ExecuteTime": {
     "end_time": "2024-02-27T09:31:32.262608174Z",
     "start_time": "2024-02-27T09:31:30.485211391Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 09:31:30.671677: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-27 09:31:30.673002: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-27 09:31:30.692315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-27 09:31:30.692334: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-27 09:31:30.692890: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-27 09:31:30.696115: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-27 09:31:30.696700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 09:31:31.177818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/zeeshan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.15.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.16.1\n",
      "GPU is NOT AVAILABLE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 09:31:32.253769: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 09:31:32.258670: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Download the IMDB dataset\n",
    "\n",
    "The IMDB dataset is available on [TensorFlow datasets](https://github.com/tensorflow/datasets). The following code downloads the IMDB dataset to your machine (or the colab runtime):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zXXx5Oc3pOmN",
    "vscode": {
     "languageId": "python"
    },
    "ExecuteTime": {
     "end_time": "2024-02-27T09:32:12.956621224Z",
     "start_time": "2024-02-27T09:32:11.584023725Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 09:32:11.606890: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /content/imdb_reviews/plain_text/1.0.0...\u001B[0m\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/content'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_data, test_data \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimdb_reviews\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/content\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_supervised\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m train_examples, train_labels \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mas_numpy(train_data)\n\u001B[1;32m      5\u001B[0m test_examples, test_labels \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mas_numpy(test_data)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[0;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 168\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    170\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:649\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001B[39;00m\n\u001B[1;32m    531\u001B[0m \n\u001B[1;32m    532\u001B[0m \u001B[38;5;124;03m`tfds.load` is a convenience method that:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001B[39;00m\n\u001B[1;32m    642\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    643\u001B[0m dbuilder \u001B[38;5;241m=\u001B[39m _fetch_builder(\n\u001B[1;32m    644\u001B[0m     name,\n\u001B[1;32m    645\u001B[0m     data_dir,\n\u001B[1;32m    646\u001B[0m     builder_kwargs,\n\u001B[1;32m    647\u001B[0m     try_gcs,\n\u001B[1;32m    648\u001B[0m )\n\u001B[0;32m--> 649\u001B[0m \u001B[43m_download_and_prepare_builder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdbuilder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m as_dataset_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    652\u001B[0m   as_dataset_kwargs \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:508\u001B[0m, in \u001B[0;36m_download_and_prepare_builder\u001B[0;34m(dbuilder, download, download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[1;32m    507\u001B[0m   download_and_prepare_kwargs \u001B[38;5;241m=\u001B[39m download_and_prepare_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[0;32m--> 508\u001B[0m   \u001B[43mdbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[0;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 168\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    170\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:653\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, download_dir, download_config, file_format)\u001B[0m\n\u001B[1;32m    643\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\n\u001B[1;32m    644\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNot enough disk space. Needed: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m (download: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, generated: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    645\u001B[0m       \u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    649\u001B[0m       )\n\u001B[1;32m    650\u001B[0m   )\n\u001B[1;32m    651\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_download_bytes()\n\u001B[0;32m--> 653\u001B[0m dl_manager \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_download_manager\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    654\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    655\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    656\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;66;03m# Maybe save the `builder_cls` metadata common to all builder configs.\u001B[39;00m\n\u001B[1;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBUILDER_CONFIGS:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1204\u001B[0m, in \u001B[0;36mDatasetBuilder._make_download_manager\u001B[0;34m(self, download_dir, download_config)\u001B[0m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1188\u001B[0m     max_simultaneous_downloads\n\u001B[1;32m   1189\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMAX_SIMULTANEOUS_DOWNLOADS\n\u001B[1;32m   1190\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMAX_SIMULTANEOUS_DOWNLOADS \u001B[38;5;241m<\u001B[39m max_simultaneous_downloads\n\u001B[1;32m   1191\u001B[0m ):\n\u001B[1;32m   1192\u001B[0m   logging\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m   1193\u001B[0m       (\n\u001B[1;32m   1194\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe dataset \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m sets `MAX_SIMULTANEOUS_DOWNLOADS`=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m. The\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1201\u001B[0m       download_config\u001B[38;5;241m.\u001B[39moverride_max_simultaneous_downloads,\n\u001B[1;32m   1202\u001B[0m   )\n\u001B[0;32m-> 1204\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdownload\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDownloadManager\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanual_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanual_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl_infos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl_infos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanual_dir_instructions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMANUAL_DOWNLOAD_INSTRUCTIONS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mFORCE_REDOWNLOAD\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1211\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_extraction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mFORCE_REDOWNLOAD\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1212\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_checksums_validation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforce_checksums_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[43m    \u001B[49m\u001B[43mregister_checksums\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregister_checksums\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mregister_checksums_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mregister_checksums_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1215\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverify_ssl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverify_ssl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1216\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_simultaneous_downloads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_simultaneous_downloads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:255\u001B[0m, in \u001B[0;36mDownloadManager.__init__\u001B[0;34m(self, download_dir, extract_dir, manual_dir, manual_dir_instructions, url_infos, dataset_name, force_download, force_extraction, force_checksums_validation, register_checksums, register_checksums_path, verify_ssl, max_simultaneous_downloads)\u001B[0m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_manual_dir: Optional[epath\u001B[38;5;241m.\u001B[39mPath] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    252\u001B[0m     manual_dir  \u001B[38;5;66;03m# pytype: disable=annotation-type-mismatch  # attribute-variable-annotations\u001B[39;00m\n\u001B[1;32m    253\u001B[0m )\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_manual_dir_instructions \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mdedent(manual_dir_instructions)\n\u001B[0;32m--> 255\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_dir\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmkdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extract_dir\u001B[38;5;241m.\u001B[39mmkdir(parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_force_download \u001B[38;5;241m=\u001B[39m force_download\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/etils/epath/gpath.py:203\u001B[0m, in \u001B[0;36m_GPath.mkdir\u001B[0;34m(self, mode, parents, exist_ok)\u001B[0m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Create a new directory at this given path.\"\"\"\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m parents:\n\u001B[0;32m--> 203\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_path_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexist_ok\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mmkdir(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path_str, exist_ok\u001B[38;5;241m=\u001B[39mexist_ok, mode\u001B[38;5;241m=\u001B[39mmode)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/etils/epath/backend.py:169\u001B[0m, in \u001B[0;36m_OsPathBackend.makedirs\u001B[0;34m(self, path, exist_ok, mode)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmakedirs\u001B[39m(\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    163\u001B[0m     path: PathLike,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    166\u001B[0m     mode: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    167\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    168\u001B[0m   mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0o777\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m mode\n\u001B[0;32m--> 169\u001B[0m   \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexist_ok\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/os.py:215\u001B[0m, in \u001B[0;36mmakedirs\u001B[0;34m(name, mode, exist_ok)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m head \u001B[38;5;129;01mand\u001B[39;00m tail \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path\u001B[38;5;241m.\u001B[39mexists(head):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexist_ok\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileExistsError\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m         \u001B[38;5;66;03m# Defeats race condition when another thread created the path\u001B[39;00m\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/os.py:225\u001B[0m, in \u001B[0;36mmakedirs\u001B[0;34m(name, mode, exist_ok)\u001B[0m\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 225\u001B[0m     \u001B[43mmkdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001B[39;00m\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m exist_ok \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path\u001B[38;5;241m.\u001B[39misdir(name):\n",
      "\u001B[0;31mPermissionError\u001B[0m: [Errno 13] Permission denied: '/content'"
     ]
    }
   ],
   "source": [
    "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], data_dir=\"/content\",\n",
    "                                  batch_size=-1, as_supervised=True)\n",
    "\n",
    "train_examples, train_labels = tfds.as_numpy(train_data)\n",
    "test_examples, test_labels = tfds.as_numpy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and supply relavant directory (data_dir) if you'd like to read your own dataset from a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"],\n",
    "#                                  data_dir=\"/home/ali/PycharmProjects/tfProject/dataset\",\n",
    "#                                  batch_size=-1, as_supervised=True)\n",
    "# train_examples, train_labels = tfds.as_numpy(train_data)\n",
    "# test_examples, test_labels = tfds.as_numpy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Explore the data \n",
    "\n",
    "Let's take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8qCnve_-lkO",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnKvHWW4-lkW"
   },
   "source": [
    "Let's print first 10 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtTS4kpEpjbi",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_examples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFtaCHTdc-GY"
   },
   "source": [
    "Let's also print the first 10 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvAjVXOWc6Mj",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "The neural network is created by stacking layers—this requires three main architectural decisions:\n",
    "\n",
    "* How to represent the text?\n",
    "* How many layers to use in the model?\n",
    "* How many *hidden units* to use for each layer?\n",
    "\n",
    "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
    "\n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have two advantages:\n",
    "*   we don't have to worry about text preprocessing,\n",
    "*   we can benefit from transfer learning.\n",
    "\n",
    "For this example we will use a model from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2).\n",
    "\n",
    "There are two other models to test for the sake of this tutorial:\n",
    "* [google/nnlm-en-dim50-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2) - same as [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2), but with additional text normalization to remove punctuation. This can help to get better coverage of in-vocabulary embeddings for tokens on your input text.\n",
    "* [google/nnlm-en-dim128-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) - A larger model with an embedding dimension of 128 instead of the smaller 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In2nDpTLkgKa"
   },
   "source": [
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that the output shape of the produced embeddings is a expected: `(num_examples, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NUbzVeYkgcO",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to try this model instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# model = \"https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2\"\n",
    "# hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "# hub_layer(train_examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to try this model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# model = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"\n",
    "# hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "# hub_layer(train_examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfSbV6igl1EH"
   },
   "source": [
    "Let's now build the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpKOoWgu-llD",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "The layers are stacked sequentially to build the classifier:\n",
    "\n",
    "1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The model that we are using ([google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
    "2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
    "3. The last layer is densely connected with a single output node. This outputs logits: the log-odds of the true class, according to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XMwnDOp-llH"
   },
   "source": [
    "### Hidden units\n",
    "\n",
    "The above model has two intermediate or \"hidden\" layers, between the input and output. The number of outputs (units, nodes, or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n",
    "\n",
    "If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns—patterns that improve performance on training data but not on the test data. This is called *overfitting*, and we'll explore it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
    "\n",
    "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
    "\n",
    "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
    "\n",
    "Now, configure the model to use an optimizer and a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr0GP-cQ-llN",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCWYwkug-llQ"
   },
   "source": [
    "## Create a validation set\n",
    "\n",
    "When training, we want to check the accuracy of the model on data it hasn't seen before. Create a *validation set* by setting apart 10,000 examples from the original training data. (Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NpcXY9--llS",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "x_val = train_examples[:10000]\n",
    "partial_x_train = train_examples[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model for 40 epochs in mini-batches of 512 samples. This is 40 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXSGrjWZ-llW",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOMKywn4zReN",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model predict, this is assuming that we feed a brand new data to our trained system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "results_pred = model.predict(test_examples)\n",
    "classes_x=np.argmax(results_pred ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1iEXVTR0Z2t"
   },
   "source": [
    "This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## Create a graph of accuracy and loss over time\n",
    "\n",
    "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcvSXvhp-llb",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRKsqL40-lle"
   },
   "source": [
    "There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGoYf2Js-lle",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hXx-xOv-llh",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFEmZ5zq-llk"
   },
   "source": [
    "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
    "\n",
    "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
    "\n",
    "This isn't the case for the validation loss and accuracy—they seem to peak after about twenty epochs. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations *specific* to the training data that do not *generalize* to test data.\n",
    "\n",
    "For this particular case, we could prevent overfitting by simply stopping the training after twenty or so epochs. Later, you'll see how to do this automatically with a callback."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic-text-classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
